theory SubsetSum_PneqNP
  imports SubsetSum_CookLevin
begin

text â€¹
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                             %
%      A CONDITIONAL PROOF THAT P â‰  NP FROM AN INFORMATION-FLOW PRINCIPLE     %
%                                                                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This theory completes a mechanised formalisation of the lower-bound argument for
SUBSETâ€“SUM originating in

    C. A. Feinstein,
    â€œDialogue Concerning the Two Chief World Views,â€
    arXiv:1605.08639.

The development begins from a simple and intuitively compelling informational
principle:

      To decide whether two quantities L and R are equal,
      a solver must obtain information about L and information about R.

In the SUBSETâ€“SUM setting, this principle acquires a precise combinatorial
interpretation.  For each split position k, the canonical decomposition
eâ‚–(as, s) separates the verification condition into two sets of candidate
contributions:

      LHS(eâ‚– as s)     of size 2^k,
      RHS(eâ‚– as s)     of size 2^(n âˆ’ k),

arising respectively from all prefix-choices and all suffix-choices of the
unknown vector xs.  Determining whether a solution exists is equivalent to
asking whether these two sets intersect.

If one treats the elements of LHS(eâ‚– as s) and RHS(eâ‚– as s) as *independent
candidates* generated by xs, then the information principle above implies that
in the worst case a solver must obtain enough information to distinguish
**every** canonical candidate on both sides.  Otherwise, the solver cannot rule
out the possibility that some unexamined L-value might equal some unexamined
R-value.

A natural question arises at this point:

      Why does the information principle force the solver to distinguish
      *all* members of LHS(eâ‚–) and RHS(eâ‚–)?

The reason is that, before reading any input, every prefix-choice and every
suffix-choice is a viable way for the equation to hold.  Each produces a
distinct candidate L-value or R-value, and the solver has no prior knowledge
that would allow it to ignore some possibilities.  To determine whether the two
sets intersect, the solver must rule out (or confirm) every possible L- and
R-value consistent with the instance.  Treating the candidates as independent
thus leads directly to a worst-case requirement of distinguishing all of them.

When this informational requirement is expressed within the Cookâ€“Levin
Turing-machine framework, it becomes the LR-read property: a structural
assumption asserting that, for some split k, the machineâ€™s observable behaviour
distinguishes exactly the canonical left and right candidate sets LHS(eâ‚–) and
RHS(eâ‚–).  LR-read is the single assumption needed to transfer the abstract
decision-tree lower bound to the Turing-machine model.

Under this assumption, the formalisation proves that any solver must take at
least

      2 Â· sqrt(2^n)

steps on distinct-subset-sum inputs of length n.  Since this quantity grows
faster than any polynomial, we obtain the conditional implication:

      If every polynomial-time solver for SUBSETâ€“SUM satisfies LR-read,
      then P â‰  NP.

All mathematical components except LR-read itself are fully mechanised in
Isabelle/HOL: the decision-tree adversary argument, the Cookâ€“Levin machine
semantics, and the NP verifier for SUBSETâ€“SUM.  The LR-read property is the
single explicit structural hypothesis tying these components together.

AI systems (ChatGPT and Claude) assisted in improving the exposition and
organisation of the informal text; all formal proofs are verified by Isabelle/HOL.
â€º


section â€¹1.  Why SUBSETâ€“SUM?â€º

text â€¹
Our interest in SUBSETâ€“SUM begins with a basic information principle:

      To decide whether two quantities L and R are equal,
      a solver must obtain some information about L
      and some information about R.

In SUBSETâ€“SUM, each 0/1-choice vector xs determines the value

      âˆ‘áµ¢ as!i * xs!i,

and for distinct-subset-sum instances these values are all different.
Thus each xs represents a *distinct candidate contribution* to the equation.

Splitting the sum at a position k separates these contributions into two
canonical candidate families:

      LHS(eâ‚– as s)    determined by xs[0..kâˆ’1],
      RHS(eâ‚– as s)    determined by xs[k..nâˆ’1].

If we treat the elements of LHS(eâ‚–) and RHS(eâ‚–) as *independent candidates*,
then solvability at split k is equivalent to asking whether these two sets
intersect.  Crucially, for distinct-subset-sum instances, **each element in each
set corresponds to a different feasible choice of xs**, and the solver has no
prior information about which choices are viable.

Therefore, to determine whether any intersection exists, the solver must be able
to distinguish all relevant candidates on both sides.  This informational
perspectiveâ€”the idea that the solver must gather enough data to rule out or
confirm each independent candidate L- and R-valueâ€”is what ultimately drives the
lower-bound argument in the remainder of the theory.
â€º


section â€¹2.  The Decision-Tree Lower Boundâ€º

text â€¹
Building on the informational viewpoint of Section 1, the theory
â€¹SubsetSum_DecisionTreeâ€º develops a purely combinatorial lower bound for
SUBSETâ€“SUM in an abstract â€œreaderâ€ model.

In this model, a solver gradually acquires information about the unknown
choice vector xs, while an adversary tracks all choices still compatible with
the solverâ€™s observations.  For each split k, the canonical decomposition
eâ‚–(as, s) induces the two candidate families

      LHS(eâ‚– as s)    and    RHS(eâ‚– as s),

of sizes 2^k and 2^(nâˆ’k), exactly as described in Section 1.

The abstract lower bound is derived from two axioms:

  â€¢ â€¹coverageâ€º â€” on each distinct-subset-sum instance, there exists a split k
      at which the solverâ€™s information flow matches the canonical families
      LHS(eâ‚–) and RHS(eâ‚–);

  â€¢ â€¹costâ€º â€” the solver must spend at least one unit of work to 
      distinguish each candidate value it recognises on either side.

From these two axioms alone, â€¹SubsetSum_Lemma1â€º proves the inequality

      steps(as, s)  â‰¥  2^k + 2^(n âˆ’ k),

for the appropriate split k.  Minimising the right-hand side over k yields the
tight symmetric bound

      steps(as, s)  â‰¥  2 Â· sqrt(2^n).

This argument uses only the combinatorial structure of the candidate sets;
no assumptions about Turing machines or encodings appear at this stage.
The role of the later sections is to transport this abstract lower bound into
the Cookâ€“Levin Turing-machine model.
â€º


section â€¹3.  From Decision Trees to Cookâ€“Levin Turing Machinesâ€º

text â€¹
The âˆš(2^n) lower bound established in â€¹SubsetSum_DecisionTreeâ€º applies to an
abstract reader model whose behaviour directly exposes which values in the
canonical sets LHS(eâ‚–) and RHS(eâ‚–) it has distinguished.  A Cookâ€“Levin
Turing machine, however, is far more flexible: it may reorder, copy, compress,
hash, or interleave portions of its input tape.  Because of this expressive
power, the decision-tree lower bound does not automatically transfer to
Turing machines.

To bridge this gap, the theory â€¹SubsetSum_CookLevinâ€º introduces the locale
â€¹LR_Read_TMâ€º.  Its purpose is to represent, inside the Cookâ€“Levin model,
the same left/right information structure that governs the abstract
lower bound.

Recall from Section 1 that for any split position k, the canonical decomposition
eâ‚–(as, s) produces two independent candidate sets of possible contributions:

      LHS(eâ‚– as s)     of size 2^k,
      RHS(eâ‚– as s)     of size 2^(n âˆ’ k),

arising from all prefix and suffix choices of the unknown vector xs.
Section 2 showed that any solver forced to distinguish exactly these candidates
must spend at least 2Â·sqrt(2^n) steps.

A Turing machine does not expose its information flow in this direct way.
Thus we introduce the predicates

      seenL_TM as s k,
      seenR_TM as s k,

defined by analysing how the machineâ€™s observable behaviour changes when the
input is modified in ways that alter only L-relevant information (affecting the
left side but not the right) or only R-relevant information (affecting the
right side but not the left).  These sets collectively measure what the machine
has effectively â€œlearnedâ€™â€™ about each side of the equation from the bits it has
read.

-------------------------------------------------------------------------------
â–   LR-read: matching the canonical candidate sets
-------------------------------------------------------------------------------

The LR-read hypothesis asserts that for every distinct-subset-sum instance
(as, s), there exists some split k such that

      seenL_TM as s k = LHS(eâ‚– as s)
      seenR_TM as s k = RHS(eâ‚– as s).

Thus, the machineâ€™s observable behaviour distinguishes *exactly* the canonical
left and right candidate sets produced by eâ‚–(as, s).  It does not overlook any
canonical candidates, nor does it create distinctions not justified by the
SUBSETâ€“SUM structure.

This coverage requirement ensures that the solverâ€™s information flow aligns with
the same prefix/suffix decomposition that underlies the decision-tree analysis.
However, **coverage alone is not sufficient** to transfer the lower bound:
one also needs a constraint on the *cost* of distinguishing these candidates.

-------------------------------------------------------------------------------
â–   The cost principle in the TM setting
-------------------------------------------------------------------------------

The second LR-read axiom states:

      steps_TM as s â‰¥ |seenL_TM as s k| + |seenR_TM as s k|.

That is, distinguishing each candidate value (on either side) requires at least
one unit of work for the Turing machine.

When this cost principle is combined with the coverage equalities above, we
obtain

      |seenL_TM as s k| = 2^k,
      |seenR_TM as s k| = 2^(n âˆ’ k),

and therefore the lower bound

      steps_TM as s â‰¥ 2^k + 2^(n âˆ’ k) â‰¥ 2 Â· sqrt(2^n).

Taken together, the two LR-read axioms provide exactly the structural and
quantitative conditions needed to carry the abstract decision-tree âˆš(2^n)
lower bound into the Cookâ€“Levin model.
â€º


section â€¹4.  Why LR-read is Assumedâ€º

text â€¹
The LR-read property is a modelling assumption: this theory does not attempt to
derive it for all Turing-machine solvers of SUBSETâ€“SUM.  The purpose of this
section is to explain **why** LR-read is not expected to follow from general
adversary arguments in the unrestricted Turing-machine model, and therefore why
it is introduced as an explicit structural hypothesis.

-------------------------------------------------------------------------------
â–   Why adversary arguments cannot enforce LR-read
-------------------------------------------------------------------------------

One might hope to prove LR-read by an adversary principle: if a solver does not
distinguish the canonical L-values or R-values, then an adversary should be able
to force an incorrect decision.  This reasoning succeeds in the decision-tree
(query) model, where the algorithmâ€™s access to its input is tightly restricted:
it learns only the symbols it explicitly queries, and the adversary controls the
remaining, unqueried information.

In the Turing-machine model, this strategy breaks down completely.

A Turing machine is free to inspect, copy, permute, compress, hash, or otherwise
reorganise its input tape, and may base its choices on these derived internal
representations.  Crucially, these representations are **not** visible to an
adversary, nor can an adversary influence the machineâ€™s information-flow once
the computation begins.  As a result:

  â€¢ an adversary cannot ensure that withholding or modifying particular bits of
    the *original* encoding prevents the machine from reconstructing information
    relevant to prefix-derived L-values or suffix-derived R-values; and

  â€¢ indistinguishability arguments cannot force the machine to acquire
    information according to the **canonical** L/R decomposition that underlies
    the sets â€¹LHS(eâ‚–)â€º and â€¹RHS(eâ‚–)â€º.

A useful analogy is the following. In the decision-tree model, the solver uncovers 
information one bit at a time â€” much like a player in a hidden-information card 
game (such as poker) who turns over one card while the opponent keeps the remaining 
cards concealed. The adversary can decide which cards stay hidden and which are 
revealed, and adversary proofs rely precisely on this ability to control information 
flow.

A Turing machine, by contrast, begins the game with all cards already face up: the 
entire input is visible from the start. The machine may then sort, copy, compress, and 
reorganise these cards internally in ways the adversary cannot observe or influence. 
Once all cards are face up, the adversary has no strategic power left â€” the solver 
can examine and process the information however it chooses.

This captures a general limitation of adversary-style lower bounds for
unrestricted Turing machines: they cannot mandate *how* information must be
obtained.  Structural constraints like LR-read cannot be enforced by adversary
arguments alone and must, therefore, be introduced explicitly. 
â€º


section â€¹5.  Logical Structureâ€º

text â€¹
The development is organised in three layers:

  (1) Lower-bound kernel â€” *proved*
      Theories â€¹SubsetSum_DecisionTreeâ€º and â€¹SubsetSum_Lemma1â€º prove a
      âˆš(2^n) lower bound under abstract L/R-information axioms.

  (2) Cookâ€“Levin bridge â€” *proved*
      The locale â€¹LR_Read_TMâ€º formalises how a Turing machine induces the
      distinguishability sets â€¹seenL_TMâ€º and â€¹seenR_TMâ€º required by the
      abstract lemma.

  (3) Modeling assumption â€” not proved
      Every Cookâ€“Levin Turing-machine solver for SUBSETâ€“SUM (with encoding enc0)
      satisfies LR-read.

Together these yield the conditional implication:

      If SUBSETâ€“SUM âˆˆ P and all solvers satisfy LR-read,
      then P â‰  NP.
â€º


section â€¹6.  Relation to Feinstein (2016)â€º

text â€¹
Feinsteinâ€™s 2016 paper emphasises an informational viewpoint: verifying a
candidate equality requires examining contributions from both sides of a
structurally decomposed expression.  For SUBSETâ€“SUM, the canonical split eâ‚–(as,s)
makes this decomposition explicit.  Each prefix-choice of xs contributes a
potential L-value, and each suffix-choice a potential R-value; deciding whether a
solution exists is equivalent to testing whether the two resulting sets
intersect.

This formalisation captures that insight in two stages:

  â€¢ In the decision-tree setting, the sets LHS(eâ‚– as s) and RHS(eâ‚– as s) behave
    like two independent collections of candidates.  The abstract axioms of
    â€¹SubsetSum_Lemma1â€º encode the informational requirement that a solver must be
    able to distinguish each candidate on both sides in order to determine
    whether an intersection exists.  This yields the âˆš(2^n) lower bound.

  â€¢ In the Turing-machine setting, the locale â€¹LR_Read_TMâ€º provides the structural
    assumption (LR-read) that the machineâ€™s observable behaviour distinguishes
    exactly these canonical candidate sets at some split k.  This is the Turing-
    level analogue of the same informational principle.  Once LR-read is assumed,
    the decision-tree lower bound transfers automatically to Cookâ€“Levin machines.

Thus, this formal development isolates and clarifies the combinatorial core of
Feinsteinâ€™s reasoning: the exponential candidate sets arising from the canonical
prefix/suffix decomposition and the informational burden of determining whether
they intersect.  Everything except the LR-read hypothesis is formalised entirely
within Isabelle/HOL.
â€º


section â€¹7.  Perspectiveâ€º

text â€¹
This theory establishes a *conditional* proof that P â‰  NP, based on a single
information-flow principle formalised as the LR-read assumption.

The lower-bound component of the development is fully mechanised: the
decision-tree argument, the Cookâ€“Levin Turing-machine semantics, and the NP
verifier for SUBSETâ€“SUM are all proved in Isabelle/HOL without additional
hypotheses.  The only non-mechanised ingredient is the LR-read property, which
capturesâ€”in a precise mathematical formâ€”the intuitive requirement that any
solver for SUBSETâ€“SUM must obtain enough information about both the prefix and
suffix contributions to determine whether a candidate L-value can match a
candidate R-value.

Assuming LR-read, the theory derives a âˆš(2^n) lower bound for all
Turing-machine solvers of SUBSETâ€“SUM and therefore obtains the conditional
implication:

      If every polynomial-time solver for SUBSETâ€“SUM satisfies LR-read,
      then P â‰  NP.

Thus the contribution of the formalisation is twofold:

  (a) a verified, machine-checked lower-bound framework for SUBSETâ€“SUM; and  
  (b) the identification of a single, intuitively plausible information
      principle whose acceptance is sufficient to derive P â‰  NP within the
      Cookâ€“Levin Turing-machine framework.
â€º


section â€¹8.  SUBSETâ€“SUM is in NP (formalised)â€º

text â€¹
The Cookâ€“Levin AFP library does not supply SUBSETâ€“SUM âˆˆ NP by default.
Instead we obtain it from a general verifier via SS_Verifier_NP.

A verifier provides:

  â€¢ encodings of instances and certificates,
  â€¢ a polynomial-time TM verifier V,
  â€¢ soundness and completeness.

From this we derive:

      SUBSETSUM_lang enc0 âˆˆ ğ’©ğ’«.
â€º

lemma SUBSETSUM_in_NP_global:
  assumes "SS_Verifier_NP k G V p T fverify enc0 enc_cert"
  shows "SUBSETSUM_lang enc0 âˆˆ ğ’©ğ’«"
  using SUBSETSUM_in_NP_from_verifier[OF assms] .


section â€¹9.  Definition of P = NPâ€º

definition P_eq_NP :: bool where
  "P_eq_NP âŸ· (âˆ€L::language. (L âˆˆ ğ’«) = (L âˆˆ ğ’©ğ’«))"


section â€¹10.  Bridging P to a concrete CL solverâ€º

text â€¹
If SUBSETâ€“SUM âˆˆ P, then some Cookâ€“Levin machine solves it in polynomial time.

This step passes from language complexity to concrete machine semantics.
The solverâ€™s encoding need not match the verifierâ€™s encoding; only the language
matters.
â€º

definition P_impl_CL_SubsetSum_Solver ::
  "(int list â‡’ int â‡’ string) â‡’ bool" where
  "P_impl_CL_SubsetSum_Solver enc0 âŸ·
     (SUBSETSUM_lang enc0 âˆˆ ğ’« âŸ¶
        (âˆƒM q0 enc.
           CL_SubsetSum_Solver M q0 enc âˆ§
           polytime_CL_machine M enc))"


section â€¹11.  LR-read-all-solvers hypothesisâ€º

text â€¹
This is the single modelling assumption.

For a fixed encoding enc0:

      LR_read_all_solvers_hypothesis enc0

means:

  (1) If SUBSETâ€“SUM âˆˆ P, a polynomial-time CL solver exists, and
  (2) Every CL solver satisfies LR-read, i.e. belongs to â€¹LR_Read_TMâ€º.

NP-membership is not assumed; it is proved separately.
â€º

definition LR_read_all_solvers_hypothesis ::
  "(int list â‡’ int â‡’ string) â‡’ bool" where
  "LR_read_all_solvers_hypothesis enc0 âŸ·
     P_impl_CL_SubsetSum_Solver enc0 âˆ§
     (âˆ€M q0 enc.
        CL_SubsetSum_Solver M q0 enc âŸ¶
          (âˆƒseenL seenR. LR_Read_TM M q0 enc seenL seenR))"


section â€¹12.  Core Conditional Theoremâ€º

text â€¹
This theorem expresses the logical core:

    LR assumptions  +  SUBSETâ€“SUM âˆˆ NP   â‡’   P â‰  NP.

Proof sketch:

    Assume P = NP.
    Then SUBSETâ€“SUM âˆˆ P.
    So a polynomial-time CL solver M exists.
    LR-read applies to M, giving a âˆš(2^n) lower bound.
    Contradiction with the polynomial-time upper bound.
â€º

lemma P_neq_NP_if_LR_read_all_solvers_hypothesis:
  fixes enc0 :: "int list â‡’ int â‡’ string"
  assumes H:       "LR_read_all_solvers_hypothesis enc0"
  assumes NP_enc0: "SUBSETSUM_lang enc0 âˆˆ ğ’©ğ’«"
  shows "Â¬ P_eq_NP"
proof -
  from H have
    bridge_P: "P_impl_CL_SubsetSum_Solver enc0" and
    all_LR:   "âˆ€M q0 enc.
                 CL_SubsetSum_Solver M q0 enc âŸ¶
                   (âˆƒseenL seenR. LR_Read_TM M q0 enc seenL seenR)"
    unfolding LR_read_all_solvers_hypothesis_def by blast+

  show "Â¬ P_eq_NP"
  proof
    assume eq: "P_eq_NP"

    have eq_PNP_inst:
      "(SUBSETSUM_lang enc0 âˆˆ ğ’«) = (SUBSETSUM_lang enc0 âˆˆ ğ’©ğ’«)"
      using eq unfolding P_eq_NP_def by simp

    have inP_SUBSETSUM: "SUBSETSUM_lang enc0 âˆˆ ğ’«"
      using NP_enc0 eq_PNP_inst by simp

    from bridge_P[unfolded P_impl_CL_SubsetSum_Solver_def] inP_SUBSETSUM
    obtain M q0 enc where
      solver: "CL_SubsetSum_Solver M q0 enc" and
      poly:   "polytime_CL_machine M enc"
      by blast

    from all_LR solver obtain seenL seenR where lr:
      "LR_Read_TM M q0 enc seenL seenR"
      by blast

    interpret LR: LR_Read_TM M q0 enc seenL seenR
      by (rule lr)

    from poly obtain c d where
      cpos: "c > 0" and
      bound_all: "âˆ€as s.
                    steps_CL M (enc as s)
                      â‰¤ nat (ceiling (c * (real (length as)) ^ d))"
      unfolding polytime_CL_machine_def by blast

    have family_bound:
      "âˆƒ(c::real)>0. âˆƒd::nat.
         âˆ€as s. distinct_subset_sums as âŸ¶
           steps_CL M (enc as s)
             â‰¤ nat (ceiling (c * (real (length as)) ^ d))"
      using cpos bound_all by blast

    from LR.no_polytime_CL_on_distinct_family family_bound
    show False by blast
  qed
qed


section â€¹13.  Final Packaged Theoremâ€º

text â€¹
This theorem gives the final wrapped statement:

      LR hypothesis + SUBSETâ€“SUM verifier â‡’ P â‰  NP.
â€º

theorem P_neq_NP_under_LR_model:
  fixes enc0 :: "int list â‡’ int â‡’ string"
  assumes LR: "LR_read_all_solvers_hypothesis enc0"
  assumes V:  "SS_Verifier_NP k G V p T fverify enc0 enc_cert"
  shows "Â¬ P_eq_NP"
proof -
  have NP_enc0: "SUBSETSUM_lang enc0 âˆˆ ğ’©ğ’«"
    using SUBSETSUM_in_NP_global[OF V] .
  from P_neq_NP_if_LR_read_all_solvers_hypothesis[OF LR NP_enc0]
  show "Â¬ P_eq_NP" .
qed

end
